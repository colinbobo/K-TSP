{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "xnE7p2YEuWml",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9322cf82-2316-4ff2-c042-f27ad4a5e173"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/Spring 2025/Directed Study/')\n",
        "print(os.getcwd()) # This prints the current working directory\n",
        "print(os.listdir()) # This prints the files in the current working directory"
      ],
      "metadata": {
        "id": "OLQE1vIFs9Kl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ede920ba-565a-4e9b-ae8c-d6765c786bbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Spring 2025/Directed Study\n",
            "['Directed-Study-Application.pdf', 'SyllabusBobocea.pdf', 'My Directed Study Question Answers.gdoc', 'My Directed Study Question Answers.pdf', 'TM2_project.ipynb', 'Test code', 'Copy of k-TSP.ipynb', 'Shared Directory', 'Final k-TSP.ipynb', 'SST_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as plt\n",
        "from copy import deepcopy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "SeAhS1Bps31q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of K-TSP Classifier\n",
        "\n",
        "### Intuitive Explanation:\n",
        "The basic idea is to find pairs of features that give the most information. This is done by scoring the features whose relative difference changes the most from one class to another. Thus their difference can indicate what class we should predict\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Though the algorithm's pseudocode is laid out in the figure, below are some extra details that might not be explicitly stated in the paper that guided my implementation. The first and maybe most important step was calculating all the matrices that the algorithm required.\n",
        "### Calculate all necessary matrices:\n",
        "* The first matrix we need is the rank matrix $R \\in \\mathbb{R}^\n",
        "{PxN}$. Our data $X \\in \\mathbb{R}^{PxN}$ has the same shape as well. The rank assigned to each entry in $X$ is simply calculated by assigning the highest rank to the highest value in that profile (profiles are columns here).\n",
        "* For our score matrix $\\Delta$ we have the formula $\\Delta_{ij} = |p_{ij}(C_1) - p_{ij}(C_2)|$, where $C_m$ are the possible classes for $m \\in {1,2}$, and $p_{ij}(C_m) = Prob(R_i < R_j | Y = C_m)$.\n",
        "  * This probability is calculated by the relative frequencies of the ranks of certain genes $i$ and $j$ within profiles of a given label, that is:\n",
        "  $p_{ij}(C_m) = \\frac{\\sum_{k=1}^N \\mathbb{1}(y_k = C_m) \\cdot \\mathbb{1}(R_{i,k} < R_{j,k})}{\\sum_{k=1}^N \\mathbb{1}(y_k = C_m)}$, where $\\mathbb{1}(\\cdot)$ is the indicator function. In keeping with the notation of the paper, this is alternatively expressed as, $p_{ij}(C_m) = \\frac{\\sum_{n \\in C_m} \\mathbb{1}(R_{i,n} < R_{j,n})}{|C_m|}$ where $|C_m|$ is the number of samples in $C_m$.\n",
        "* Lastly we have the \"rank score\" matrix $\\Gamma$ that is used to break ties. Each entry is calculates as follows, $\\Gamma_{ij} = |\\gamma_{ij}(C_1) = \\gamma_{ij}(C_2)|$ where $\\gamma_{ij}  = \\frac{\\sum_{n \\in C_m} (R_{i,n} - R_{j,n})}{|C_m|}$.\n",
        "\n",
        "These were the explicit formulas I used to be able to implement the algorithm, the rest of the algorithm is written clearly in the pseudocode of the paper.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ehOSVJo9dPtP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1qOdbZJwK0j"
      },
      "outputs": [],
      "source": [
        "# Algorithm for k-TSP\n",
        "class KTSP:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.data = None\n",
        "    self.labels = None\n",
        "    self.rank_matrix = None\n",
        "    self.best_pair = None\n",
        "    self.top_k_pairs = None\n",
        "\n",
        "  def compute_Rank(self, reduced_data : pd.DataFrame):\n",
        "    \"\"\"\n",
        "    for i in range(reduced_data.shape[1]):\n",
        "      sorted_col = reduced_data[i].sort_values()\n",
        "      rank = 1\n",
        "      for num in sorted_col:\n",
        "        reduced_data[i].loc[num] = rank\n",
        "        rank += 1\n",
        "    \"\"\"\n",
        "    # faster way based on built-in pandas method\n",
        "    self.rank_matrix = reduced_data.rank(method=\"first\", ascending=False, axis=0)  # Compute ranks per column\n",
        "    return self.rank_matrix.astype(int)\n",
        "\n",
        "\n",
        "  def compute_Delta(self):\n",
        "    # initializing the delta matrix, it's P X P symmetric\n",
        "    delta =  np.zeros((self.P,self.P))\n",
        "\n",
        "    # we subset the matrix based on classes\n",
        "    class_0 = self.rank_matrix.loc[:, self.labels == 0]\n",
        "    class_1 = self.rank_matrix.loc[:, self.labels == 1]\n",
        "\n",
        "    for i in range(self.P):\n",
        "      for j in range(i+1,self.P):\n",
        "        p_class0 = (class_0.iloc[i] < class_0.iloc[j]).mean()\n",
        "        p_class1 = (class_1.iloc[i] < class_1.iloc[j]).mean()\n",
        "        delta[i, j] = abs(p_class0 - p_class1)\n",
        "        delta[j, i] = delta[i, j]\n",
        "\n",
        "    return pd.DataFrame(delta)\n",
        "\n",
        "  def compute_Gamma(self):\n",
        "    # initializing the gamma matrix which is also P X P\n",
        "    gamma_mat = np.zeros((self.P,self.P))\n",
        "\n",
        "    # check that this gets all columns with the corresponding label\n",
        "    class_0 = self.rank_matrix.loc[:, self.labels == 0].to_numpy()\n",
        "    class_1 = self.rank_matrix.loc[:, self.labels == 1].to_numpy()\n",
        "\n",
        "    for i in range(self.P):\n",
        "      for j in range(i+1, self.P):\n",
        "        #check this\n",
        "        gamma_0 = np.mean(class_0[i, :] - class_0[j, :])\n",
        "        gamma_1 = np.mean(class_1[i, :] - class_1[j, :])\n",
        "        gamma_mat[i, j] = abs(gamma_0 - gamma_1)\n",
        "        gamma_mat[j, i] = gamma_mat[i, j]\n",
        "\n",
        "    return pd.DataFrame(gamma_mat)\n",
        "\n",
        "\n",
        "  def get_ordered_list(self, delta_mat, gamma_mat):\n",
        "    # stack Delta and Gamma to sort pairs by Delta, then Gamma\n",
        "    delta_stacked = delta_mat.stack()\n",
        "    gamma_stacked = gamma_mat.stack()\n",
        "    pairs_df = pd.DataFrame({\n",
        "        'delta': delta_stacked,\n",
        "        'gamma': gamma_stacked\n",
        "    })\n",
        "    # sort by Delta, then Gamma\n",
        "    pairs_df = pairs_df.sort_values(by=['delta', 'gamma'], ascending=[False, False])\n",
        "    # select top k pairs\n",
        "    top_pairs = [(i, j) for i, j in pairs_df.index if i < j]\n",
        "    return top_pairs\n",
        "\n",
        "\n",
        "  def remove_pair(self, top_pair, list_O):\n",
        "    return [(i,j) for (i,j) in list_O if i not in top_pair and j not in top_pair]\n",
        "\n",
        "\n",
        "  def cross_val_k(self):\n",
        "    k_upper_bound = 10 # should be even, since it'll exclude the last number in the loop below\n",
        "    N = self.labels.shape[0]\n",
        "    little_n = 3\n",
        "    m = N // little_n\n",
        "    sample_indices = np.arange(self.data.shape[1])\n",
        "    np.random.shuffle(sample_indices)\n",
        "    error_rates = {}\n",
        "\n",
        "    for fold in range(m):\n",
        "      test_indices = sample_indices[fold*little_n : (fold+1)*little_n]\n",
        "      train_indices = np.setdiff1d(sample_indices, test_indices)\n",
        "\n",
        "      X_train = self.data.iloc[:, train_indices]\n",
        "      X_test = self.data.iloc[:, test_indices]\n",
        "      y_train = self.true_labels.iloc[train_indices]\n",
        "      y_test = self.true_labels.iloc[test_indices]\n",
        "\n",
        "      self.labels = pd.Series(y_train.values, index=X_train.columns)\n",
        "      self.P = X_train.shape[0]\n",
        "      self.compute_Rank(X_train)\n",
        "      delta_mat = self.compute_Delta()\n",
        "      gamma_mat = self.compute_Gamma()\n",
        "\n",
        "      list_O = self.get_ordered_list(delta_mat, gamma_mat)\n",
        "      list_Theta = []\n",
        "      for k in range(1,k_upper_bound):\n",
        "        list_Theta.append(list_O[0])\n",
        "        list_O = self.remove_pair(list_O[0], list_O)\n",
        "        if k % 2 != 0:\n",
        "          self.top_k_pairs = list_Theta\n",
        "          self.optimal_k = k\n",
        "          if self.optimal_k == 1:\n",
        "            self.best_pair = self.top_k_pairs[0]\n",
        "          predictions = self.predict(X_test)\n",
        "          accuracy = (predictions == y_test.values).mean()\n",
        "          # print(f\"Accuracy for k={k} is: {accuracy}\")\n",
        "          error_rate = 1 - accuracy\n",
        "          error_rates[k] = error_rate\n",
        "\n",
        "    self.optimal_k = min(error_rates, key=error_rates.get)\n",
        "    self.labels = pd.Series(self.true_labels.values, index=self.data.columns)\n",
        "    self.P = self.data.shape[0]\n",
        "    self.compute_Rank(self.data)\n",
        "    delta_mat = self.compute_Delta()\n",
        "    gamma_mat = self.compute_Gamma()\n",
        "    ordered_list = self.get_ordered_list(delta_mat, gamma_mat)\n",
        "    self.top_k_pairs = []\n",
        "    for _ in range(self.optimal_k):\n",
        "      self.top_k_pairs.append(ordered_list[0])\n",
        "      ordered_list = self.remove_pair(ordered_list[0], ordered_list)\n",
        "    return self.optimal_k\n",
        "\n",
        "  def fit(self, data, labels, k_cross_val=True, k=None, verbose=True):\n",
        "    self.true_labels = deepcopy(labels)\n",
        "    self.k_cross_val = k_cross_val\n",
        "    if (self.k_cross_val == False and (k == None or k == 1)) or k == 1:\n",
        "      self.optimal_k = 1\n",
        "      self.data = data\n",
        "      self.labels = labels\n",
        "      # this makes sure that we have the correct orientation for the data\n",
        "      if len(self.labels) == self.data.shape[0]:\n",
        "          self.data = self.data.T\n",
        "      elif len(self.labels) != self.data.shape[1]:\n",
        "          raise Exception(f\"Number of samples in data ({self.data.shape[1]}) must match length of labels ({len(self.labels)})\")\n",
        "\n",
        "      self.labels = pd.Series(self.labels.values, index=self.data.columns)\n",
        "      self.P = self.data.shape[0]\n",
        "      self.compute_Rank(self.data)\n",
        "      delta_mat = self.compute_Delta()\n",
        "      gamma_mat = self.compute_Gamma()\n",
        "\n",
        "      delta_arr = delta_mat.to_numpy()\n",
        "      max_delta = delta_arr.max()\n",
        "      max_indices = np.where(delta_arr == max_delta)\n",
        "      max_pairs = list(zip(max_indices[0], max_indices[1]))\n",
        "\n",
        "      if len(max_pairs) == 1:\n",
        "        return max_pairs[0]\n",
        "      else:\n",
        "        max_gamma = -float('inf')\n",
        "        self.best_pair = None\n",
        "        for i, j in max_pairs:\n",
        "            gamma_value = gamma_mat.iloc[i, j]\n",
        "            if gamma_value > max_gamma:\n",
        "                max_gamma = gamma_value\n",
        "                self.best_pair = (i, j)\n",
        "        return self.best_pair\n",
        "    else:\n",
        "      self.data = data\n",
        "      self.labels = labels\n",
        "      if k == None:\n",
        "        self.cross_val_k()\n",
        "        if verbose:\n",
        "          print(f\"Optimal k: {self.optimal_k}\")\n",
        "          print(\"Best pair:\", self.top_k_pairs)\n",
        "          print(\"Best pair:\", self.best_pair)\n",
        "      else:\n",
        "        self.optimal_k = k\n",
        "        self.labels = pd.Series(self.true_labels.values, index=self.data.columns)\n",
        "        self.P = self.data.shape[0]\n",
        "        self.compute_Rank(self.data)\n",
        "        delta_mat = self.compute_Delta()\n",
        "        gamma_mat = self.compute_Gamma()\n",
        "        ordered_list = self.get_ordered_list(delta_mat, gamma_mat)\n",
        "        self.top_k_pairs = []\n",
        "        for _ in range(self.optimal_k):\n",
        "          self.top_k_pairs.append(ordered_list[0])\n",
        "          ordered_list = self.remove_pair(ordered_list[0], ordered_list)\n",
        "\n",
        "\n",
        "\n",
        "  def predict(self, new_samples):\n",
        "    if isinstance(new_samples, pd.Series):\n",
        "        new_samples = new_samples.to_frame().T\n",
        "    if (self.k_cross_val == False and (self.optimal_k == None or self.optimal_k == 1)) or self.optimal_k == 1:\n",
        "      i = self.best_pair[0]\n",
        "      j = self.best_pair[1]\n",
        "\n",
        "      # compute p_ij(C_m)\n",
        "      class_0 = self.rank_matrix.loc[:, self.labels == 0]\n",
        "      class_1 = self.rank_matrix.loc[:, self.labels == 1]\n",
        "      p_class0 = (class_0.iloc[i] < class_0.iloc[j]).mean()\n",
        "      p_class1 = (class_1.iloc[i] < class_1.iloc[j]).mean()\n",
        "\n",
        "      # decision rule\n",
        "      predictions = []\n",
        "      for _, sample in new_samples.items():\n",
        "        new_sample = pd.Series(sample, index=self.data.index)\n",
        "        new_ranks = pd.Series(new_sample).rank(method=\"first\", ascending=False)\n",
        "        votes = {0: 0, 1: 0}\n",
        "        if p_class0 > p_class1:\n",
        "          if new_ranks.iloc[i] < new_ranks.iloc[j]:\n",
        "            votes[0] += 1\n",
        "          else:\n",
        "            votes[1] += 1\n",
        "        else:\n",
        "          if new_ranks.iloc[i] < new_ranks.iloc[j]:\n",
        "            votes[1] += 1\n",
        "          else:\n",
        "            votes[0] += 1\n",
        "        predictions.append(0 if votes[0] > votes[1] else 1)\n",
        "      return np.array(predictions)\n",
        "    else:\n",
        "      if self.top_k_pairs is None:\n",
        "        raise Exception(\"Must call fit() to select top k pairs before making predictions\")\n",
        "\n",
        "      predictions = []\n",
        "      for _, sample in new_samples.items():\n",
        "        new_sample = pd.Series(sample, index=self.data.index)\n",
        "        new_ranks = pd.Series(new_sample).rank(method=\"first\", ascending=False)\n",
        "        votes = {0: 0, 1: 0}\n",
        "        for i, j in self.top_k_pairs:\n",
        "          class_0 = self.rank_matrix.loc[:, self.labels == 0]\n",
        "          class_1 = self.rank_matrix.loc[:, self.labels == 1]\n",
        "          p_class0 = (class_0.iloc[i] < class_0.iloc[j]).mean()\n",
        "          p_class1 = (class_1.iloc[i] < class_1.iloc[j]).mean()\n",
        "          if p_class0 > p_class1:\n",
        "            if new_ranks.iloc[i] < new_ranks.iloc[j]:\n",
        "                votes[0] += 1\n",
        "            else:\n",
        "                votes[1] += 1\n",
        "          else:\n",
        "            if new_ranks.iloc[i] < new_ranks.iloc[j]:\n",
        "                votes[1] += 1\n",
        "            else:\n",
        "                votes[0] += 1\n",
        "        predictions.append(0 if votes[0] > votes[1] else 1)\n",
        "      return np.array(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_process(filename, n_features):\n",
        "  pros = pd.read_csv(filename, delimiter=',', header=None, low_memory=False)\n",
        "  X = pros.iloc[1:]\n",
        "  X = X.apply(pd.to_numeric, errors='coerce')\n",
        "  y = pros.iloc[0]\n",
        "  y = y.replace({'Tumor': 1, 'Normal': 0})\n",
        "  print(f\"Original data shape for X: {X.shape} and y: {y.shape}\")\n",
        "\n",
        "  # The code below is only if you want to limit the amount of features used\n",
        "\n",
        "  num_features_to_keep = n_features\n",
        "  print(f\"Keeping the top {num_features_to_keep} features with the highest variance\")\n",
        "  variances = X.var(axis=1)\n",
        "  top_features = variances.nlargest(num_features_to_keep).index\n",
        "  X = X.loc[top_features]\n",
        "  y = y.iloc[:X.shape[1]]\n",
        "\n",
        "  return X, y"
      ],
      "metadata": {
        "id": "x3V6CEjlh3iO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def jitter(X, y):\n",
        "  # first I have to identify the minority and majority class of the dataset\n",
        "  # add code to get counts of both classes\n",
        "  y_list = list(y)\n",
        "  counts = y.value_counts().to_dict()\n",
        "  #print(\"Class counts:\", y.value_counts())\n",
        "  class_0_count = counts[0]\n",
        "  class_1_count = counts[1]\n",
        "  if class_0_count == class_1_count:\n",
        "    return X, y\n",
        "\n",
        "  # for the class that is the minority\n",
        "  # write code that takes data points at random and adds gaussian noise\n",
        "  # leave the variance visible so as to be tweaked\n",
        "  if class_0_count < class_1_count:\n",
        "    diff = class_1_count - class_0_count\n",
        "    class_0_indices = np.where(y == 0)[0]\n",
        "\n",
        "\n",
        "    # the std dev we use is one tenth of the std dev of the feature within that class\n",
        "    # IMPORTANT: SHOULD CHANGE THIS TO A FRACTION OF THE DISTANCE TO THE NEAREST NEIGHBOR, VARIANCES ARE TOO LARGE FOR FEATURES\n",
        "    std_dev = np.std(X.iloc[:, class_0_indices].to_numpy(), axis=1, keepdims=True)\n",
        "    alpha = 0.05\n",
        "\n",
        "\n",
        "    indices = np.random.choice(class_0_indices, size=diff, replace=True)\n",
        "    selected_samples = X.iloc[:, indices]\n",
        "    noise = np.random.normal(0.0, alpha * std_dev, selected_samples.shape)\n",
        "    jittered_samples = selected_samples + noise\n",
        "    y_list.extend([0] * diff)\n",
        "\n",
        "    # print(f\"Example of a jittered sample:\\n\")\n",
        "    # print(f\"Original: {selected_samples.iloc[:5, 1]} \\n\")\n",
        "    # print(f\"Jittered: {jittered_samples.iloc[:5, 1]}\")\n",
        "\n",
        "  elif class_1_count < class_0_count:\n",
        "    diff = class_0_count - class_1_count\n",
        "    class_1_indices = np.where(y == 1)[0]\n",
        "    std_dev = np.std(X.iloc[:, class_0_indices].to_numpy(), axis=1, keepdims=True)\n",
        "    alpha = 0.05\n",
        "\n",
        "\n",
        "    indices = np.random.choice(class_1_indices, size=diff, replace=True)\n",
        "    selected_samples = X.iloc[:, indices]\n",
        "    noise = np.random.normal(0, alpha * std_dev, selected_samples.shape)\n",
        "    jittered_samples = selected_samples + noise\n",
        "    y_list.extend([1] * diff)\n",
        "\n",
        "    # print(f\"Example of a jittered sample:\\n\")\n",
        "    # print(f\"Original: \\n{selected_samples.iloc[:, 1]} \\n\")\n",
        "    # print(f\"Jittered: \\n{jittered_samples.iloc[:, 1]}\")\n",
        "\n",
        "\n",
        "  X_aug = pd.concat([X, jittered_samples], axis=1)\n",
        "  y_aug = pd.Series(y_list)\n",
        "  X_aug.columns = range(0, X_aug.shape[1])\n",
        "  y_aug.index = range(0, y_aug.shape[0])\n",
        "  print(f\"New data shape for X: {X_aug.shape}, and y: {y_aug.shape}\")\n",
        "  return X_aug, y_aug\n",
        "\n"
      ],
      "metadata": {
        "id": "8uL3hF0l_99H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_split(X, y):\n",
        "  # can modify test_size and var\n",
        "  X, y = jitter(X, y)\n",
        "  return train_test_split(X.T, y, test_size=0.3)"
      ],
      "metadata": {
        "id": "4WO4TmL4iYLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_report(X_train, X_test, y_train, y_test):\n",
        "  X_train = X_train.T\n",
        "  X_test = X_test.T\n",
        "  tsp = KTSP()\n",
        "  best_pair = tsp.fit(X_train, y_train, k_cross_val=True, verbose=True)\n",
        "  y_pred = tsp.predict(X_test)\n",
        "  return classification_report(y_test, y_pred, output_dict=True)"
      ],
      "metadata": {
        "id": "oLgz-b3Mi3wm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_val(X, y, k=3):\n",
        "  for i in range(k):\n",
        "    print(f\"Fold: {i+1}\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X.T, y, test_size=0.2)\n",
        "    report = train_and_report(X_train, X_test, y_train, y_test)\n",
        "    print(f\"Fold {i+1} Report:\\n {report}\")\n",
        "    if i == 0:\n",
        "      averages = deepcopy(report)\n",
        "    else:\n",
        "      for key in report:\n",
        "        if key == 'accuracy':\n",
        "          averages['accuracy'] += report['accuracy']\n",
        "          continue\n",
        "        if key == 'recall':\n",
        "          averages['recall'] += report['recall']\n",
        "          continue\n",
        "        for metric in report[key]:\n",
        "            averages[key][metric] += report[key][metric]\n",
        "\n",
        "\n",
        "  for key in averages:\n",
        "    if key == 'accuracy':\n",
        "          averages['accuracy'] = averages['accuracy'] / k\n",
        "          continue\n",
        "    for metric in averages[key]:\n",
        "         averages[key][metric] = averages[key][metric] / k\n",
        "\n",
        "  return averages\n"
      ],
      "metadata": {
        "id": "pU_srZuP5-Ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_classification_report(report):\n",
        "    lines = []\n",
        "    headers = [\"precision\", \"recall\", \"f1-score\", \"support\"]\n",
        "    line_fmt = \"{:>12} {:>10} {:>10} {:>10} {:>10}\"\n",
        "    lines.append(line_fmt.format(\"\", *headers))\n",
        "    for label in report:\n",
        "        if isinstance(report[label], dict):\n",
        "            values = [report[label].get(h, 0) for h in headers]\n",
        "            lines.append(line_fmt.format(label, *[\"{:.4f}\".format(v) if isinstance(v, float) else str(v) for v in values]))\n",
        "        elif label == \"accuracy\":\n",
        "            lines.append(\"{:>12} {:>10.4f}\".format(\"accuracy\", report[label]))\n",
        "    return \"\\n\".join(lines)"
      ],
      "metadata": {
        "id": "ql0Sl0nRPZ4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_pipeline(filename):\n",
        "  n_features = 100\n",
        "  X, y = pre_process(filename, n_features)\n",
        "  report = cross_val(X, y)\n",
        "  print(format_classification_report(report))"
      ],
      "metadata": {
        "id": "2-l115epzFL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.iloc[21]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "49BEabz6fLgv",
        "outputId": "821772e8-d8f1-4113-9742-9dd113b8078c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     301.87653\n",
              "1     301.16852\n",
              "2     301.13016\n",
              "3     301.82733\n",
              "4     301.82360\n",
              "        ...    \n",
              "85    301.27988\n",
              "86    301.24844\n",
              "87    301.29430\n",
              "88    301.12796\n",
              "89    302.34662\n",
              "Name: Sep_atlantic, Length: 90, dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sep_atlantic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>301.87653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>301.16852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>301.13016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>301.82733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>301.82360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>301.27988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>301.24844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>301.29430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>301.12796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>302.34662</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>90 rows Ã— 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"SST_data\")\n",
        "X = df.iloc[:, 1:-2]\n",
        "y_train = df.iloc[:, -1] # Keep y as a pandas Series\n",
        "\n",
        "X_train = X.T\n",
        "tsp = KTSP()\n",
        "best_pair = tsp.fit(X_train, y_train, k_cross_val=True, verbose=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jo4cheZ0erOC",
        "outputId": "3abf8df5-216c-4a2d-89ac-77cd807e118a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal k: 5\n",
            "Best pair: [(37, 39), (36, 38), (7, 24), (8, 26), (21, 22)]\n",
            "Best pair: (37, 38)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"SST_data\")\n",
        "X = df.iloc[:, 1:-2]\n",
        "y = df.iloc[:, -1] # Keep y as a pandas Series\n",
        "\n",
        "X = X.T\n",
        "report = cross_val(X, y)\n",
        "print(format_classification_report(report))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aIkqwOMcPNm",
        "outputId": "17cdc2b5-89b5-4b35-bfeb-be6ffcdde825"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold: 1\n",
            "Optimal k: 1\n",
            "Fold 1 Report:\n",
            " {'0': {'precision': 0.8333333333333334, 'recall': 0.625, 'f1-score': 0.7142857142857143, 'support': 8.0}, '1': {'precision': 0.75, 'recall': 0.9, 'f1-score': 0.8181818181818182, 'support': 10.0}, 'accuracy': 0.7777777777777778, 'macro avg': {'precision': 0.7916666666666667, 'recall': 0.7625, 'f1-score': 0.7662337662337663, 'support': 18.0}, 'weighted avg': {'precision': 0.7870370370370371, 'recall': 0.7777777777777778, 'f1-score': 0.7720057720057719, 'support': 18.0}}\n",
            "Fold: 2\n",
            "Optimal k: 1\n",
            "Fold 2 Report:\n",
            " {'0': {'precision': 0.5, 'recall': 0.16666666666666666, 'f1-score': 0.25, 'support': 6.0}, '1': {'precision': 0.6875, 'recall': 0.9166666666666666, 'f1-score': 0.7857142857142857, 'support': 12.0}, 'accuracy': 0.6666666666666666, 'macro avg': {'precision': 0.59375, 'recall': 0.5416666666666666, 'f1-score': 0.5178571428571428, 'support': 18.0}, 'weighted avg': {'precision': 0.625, 'recall': 0.6666666666666666, 'f1-score': 0.6071428571428572, 'support': 18.0}}\n",
            "Fold: 3\n",
            "Optimal k: 1\n",
            "Fold 3 Report:\n",
            " {'0': {'precision': 0.5714285714285714, 'recall': 0.6666666666666666, 'f1-score': 0.6153846153846154, 'support': 6.0}, '1': {'precision': 0.8181818181818182, 'recall': 0.75, 'f1-score': 0.782608695652174, 'support': 12.0}, 'accuracy': 0.7222222222222222, 'macro avg': {'precision': 0.6948051948051948, 'recall': 0.7083333333333333, 'f1-score': 0.6989966555183946, 'support': 18.0}, 'weighted avg': {'precision': 0.735930735930736, 'recall': 0.7222222222222222, 'f1-score': 0.7268673355629878, 'support': 18.0}}\n",
            "              precision     recall   f1-score    support\n",
            "           0     0.6349     0.4861     0.5266     6.6667\n",
            "           1     0.7519     0.8556     0.7955    11.3333\n",
            "    accuracy     0.7222\n",
            "   macro avg     0.6934     0.6708     0.6610    18.0000\n",
            "weighted avg     0.7160     0.7222     0.7020    18.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_pipeline(\"Prostate1.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tm8bJ1od0HI4",
        "outputId": "cae7b925-fff5-4e87-94b3-8decfdd30cb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-71-130fb631c538>:6: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  y = y.replace({'Tumor': 1, 'Normal': 0})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original data shape for X: (12600, 102) and y: (102,)\n",
            "Keeping the top 100 features with the highest variance\n",
            "Fold: 1\n",
            "New data shape for X: (100, 104), and y: (104,)\n",
            "Optimal k: 9\n",
            "Fold 1 Report:\n",
            " {'0': {'precision': 0.7857142857142857, 'recall': 0.8461538461538461, 'f1-score': 0.8148148148148148, 'support': 13.0}, '1': {'precision': 0.8888888888888888, 'recall': 0.8421052631578947, 'f1-score': 0.8648648648648649, 'support': 19.0}, 'accuracy': 0.84375, 'macro avg': {'precision': 0.8373015873015872, 'recall': 0.8441295546558705, 'f1-score': 0.8398398398398399, 'support': 32.0}, 'weighted avg': {'precision': 0.8469742063492063, 'recall': 0.84375, 'f1-score': 0.8445320320320321, 'support': 32.0}}\n",
            "              precision     recall   f1-score    support\n",
            "           0     0.7857     0.8462     0.8148    13.0000\n",
            "           1     0.8889     0.8421     0.8649    19.0000\n",
            "    accuracy     0.8438\n",
            "   macro avg     0.8373     0.8441     0.8398    32.0000\n",
            "weighted avg     0.8470     0.8438     0.8445    32.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_pipeline(\"Colon.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fu_C27ggMJ-R",
        "outputId": "22a958a3-3625-4cf1-8fb5-b3a4a92fdf59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-80-87a923ab50a1>:7: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  y = y.replace({'Tumor': 1, 'Normal': 0})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original data shape for X: (2000, 62) and y: (62,)\n",
            "Keeping the top 100 features with the highest variance\n",
            "Fold: 1\n",
            "New data shape for X: (100, 80), and y: (80,)\n",
            "Optimal k: 1\n",
            "Fold 1 Report:\n",
            " {'0': {'precision': 0.625, 'recall': 0.9090909090909091, 'f1-score': 0.7407407407407407, 'support': 11.0}, '1': {'precision': 0.875, 'recall': 0.5384615384615384, 'f1-score': 0.6666666666666666, 'support': 13.0}, 'accuracy': 0.7083333333333334, 'macro avg': {'precision': 0.75, 'recall': 0.7237762237762237, 'f1-score': 0.7037037037037037, 'support': 24.0}, 'weighted avg': {'precision': 0.7604166666666666, 'recall': 0.7083333333333334, 'f1-score': 0.7006172839506172, 'support': 24.0}}\n",
            "Fold: 2\n",
            "New data shape for X: (100, 80), and y: (80,)\n",
            "Optimal k: 1\n",
            "Fold 2 Report:\n",
            " {'0': {'precision': 0.8, 'recall': 0.8, 'f1-score': 0.8, 'support': 10.0}, '1': {'precision': 0.8571428571428571, 'recall': 0.8571428571428571, 'f1-score': 0.8571428571428571, 'support': 14.0}, 'accuracy': 0.8333333333333334, 'macro avg': {'precision': 0.8285714285714285, 'recall': 0.8285714285714285, 'f1-score': 0.8285714285714285, 'support': 24.0}, 'weighted avg': {'precision': 0.8333333333333334, 'recall': 0.8333333333333334, 'f1-score': 0.8333333333333334, 'support': 24.0}}\n",
            "Fold: 3\n",
            "New data shape for X: (100, 80), and y: (80,)\n",
            "Optimal k: 1\n",
            "Fold 3 Report:\n",
            " {'0': {'precision': 0.7142857142857143, 'recall': 0.9090909090909091, 'f1-score': 0.8, 'support': 11.0}, '1': {'precision': 0.9, 'recall': 0.6923076923076923, 'f1-score': 0.782608695652174, 'support': 13.0}, 'accuracy': 0.7916666666666666, 'macro avg': {'precision': 0.8071428571428572, 'recall': 0.8006993006993006, 'f1-score': 0.791304347826087, 'support': 24.0}, 'weighted avg': {'precision': 0.8148809523809524, 'recall': 0.7916666666666666, 'f1-score': 0.7905797101449276, 'support': 24.0}}\n",
            "Fold: 4\n",
            "New data shape for X: (100, 80), and y: (80,)\n",
            "Optimal k: 1\n",
            "Fold 4 Report:\n",
            " {'0': {'precision': 0.8461538461538461, 'recall': 1.0, 'f1-score': 0.9166666666666666, 'support': 11.0}, '1': {'precision': 1.0, 'recall': 0.8461538461538461, 'f1-score': 0.9166666666666666, 'support': 13.0}, 'accuracy': 0.9166666666666666, 'macro avg': {'precision': 0.9230769230769231, 'recall': 0.9230769230769231, 'f1-score': 0.9166666666666666, 'support': 24.0}, 'weighted avg': {'precision': 0.9294871794871794, 'recall': 0.9166666666666666, 'f1-score': 0.9166666666666666, 'support': 24.0}}\n",
            "Fold: 5\n",
            "New data shape for X: (100, 80), and y: (80,)\n",
            "Optimal k: 1\n",
            "Fold 5 Report:\n",
            " {'0': {'precision': 1.0, 'recall': 0.7, 'f1-score': 0.8235294117647058, 'support': 10.0}, '1': {'precision': 0.8235294117647058, 'recall': 1.0, 'f1-score': 0.9032258064516129, 'support': 14.0}, 'accuracy': 0.875, 'macro avg': {'precision': 0.9117647058823529, 'recall': 0.85, 'f1-score': 0.8633776091081593, 'support': 24.0}, 'weighted avg': {'precision': 0.8970588235294118, 'recall': 0.875, 'f1-score': 0.8700189753320683, 'support': 24.0}}\n",
            "              precision     recall   f1-score    support\n",
            "           0     0.7971     0.8636     0.8162    10.6000\n",
            "           1     0.8911     0.7868     0.8253    13.4000\n",
            "    accuracy     0.8250\n",
            "   macro avg     0.8441     0.8252     0.8207    24.0000\n",
            "weighted avg     0.8470     0.8250     0.8222    24.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a [link](https://www.nb-data.com/p/breaking-down-the-classification) that details the breakdown/meaning of everything in the classification report"
      ],
      "metadata": {
        "id": "CKk1EYed62dY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Structure of the pipeline\n",
        "1. Start with a function that loads/pre-processes the data\n",
        "\n",
        "    a. Input: filename; Output: X, y datasets\n",
        "\n",
        "2. Then have function that takes X, y and creates stratified test and train sets (will eventually implement copy then jitter for creating extra data)\n",
        "\n",
        "    a. Input: X, y; Output: X_train, y_train, X_test, y_test\n",
        "\n",
        "3. Then I will have a function that trains on the train data, then does prediction on the test data, this will report a number of different metrics for the accuracy of the classifier\n",
        "\n",
        "    a. Input: X_train, y_train, X_test, y_test; Output: Accuracy Report\n",
        "\n",
        "4. Can have one more function that repeats steps 2 and 3 with different sets for cross-validation and then (I think) I average the reports\n"
      ],
      "metadata": {
        "id": "XXONg-4qekn9"
      }
    }
  ]
}